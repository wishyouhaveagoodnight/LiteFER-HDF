{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621e9a4e-8a94-426f-a5ba-b4fb58bd37b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torchvision.models import mobilenet_v2, MobileNet_V2_Weights\n",
    "from torchvision.ops import DeformConv2d as TVDeformConv2d\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "\n",
    "\n",
    "# --------------------------- 辅助模块：位置编码 ---------------------------\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(1, max_len, d_model)\n",
    "        pe[0, :, 0::2] = torch.sin(position * div_term)\n",
    "        pe[0, :, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: [B, L, D]\n",
    "        \"\"\"\n",
    "        B, L, D = x.shape\n",
    "        assert D == self.pe.size(2), \"特征维度不匹配\"\n",
    "        pe = self.pe[:, :L, :]\n",
    "        return x + pe.expand(B, -1, -1)\n",
    "\n",
    "\n",
    "# --------------------------- 可变形卷积模块 ---------------------------\n",
    "class DeformConv2d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, padding=1):\n",
    "        super().__init__()\n",
    "        self.offset_channels = 2 * kernel_size * kernel_size\n",
    "        self.offset_conv = nn.Conv2d(\n",
    "            in_channels,\n",
    "            self.offset_channels,\n",
    "            kernel_size=3,\n",
    "            padding=padding,\n",
    "            bias=True\n",
    "        )\n",
    "        self.offset_conv.weight.data.zero_()\n",
    "        self.offset_conv.bias.data.zero_()\n",
    "\n",
    "        self.dcn = TVDeformConv2d(\n",
    "            in_channels, out_channels, kernel_size, padding=padding\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        offset = self.offset_conv(x)  # [B, 18, H, W]\n",
    "        return self.dcn(x, offset)\n",
    "\n",
    "\n",
    "# --------------------------- Transformer 单元 ---------------------------\n",
    "class TransformerUnit(nn.Module):\n",
    "    def __init__(self, d_model):\n",
    "        super().__init__()\n",
    "        self.attention = nn.MultiheadAttention(d_model, num_heads=2, batch_first=True)\n",
    "        self.pos_enc = PositionalEncoding(d_model)\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model * 4),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model * 4, d_model)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        is_image = (x.dim() == 4)\n",
    "        if is_image:\n",
    "            B, C, H, W = x.shape\n",
    "            x = x.flatten(2).permute(0, 2, 1)  # [B, H*W, C]\n",
    "\n",
    "        x = self.pos_enc(x)\n",
    "\n",
    "        attn_out, _ = self.attention(x, x, x)\n",
    "        x = x + attn_out\n",
    "        x = x + self.ffn(self.norm(x))\n",
    "\n",
    "        if is_image:\n",
    "            x = x.permute(0, 2, 1).view(B, C, H, W)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "# --------------------------- 层级融合模块 ---------------------------\n",
    "\n",
    "# --------------------------- 新增模块 ---------------------------\n",
    "class AdaptiveFusion(nn.Module):\n",
    "     def __init__(self, channels):\n",
    "         super().__init__()\n",
    "         self.channel_att = ChannelAttention(channels)\n",
    "         self.spatial_att = SpatialAttention()\n",
    "         self.global_weight = nn.Parameter(torch.tensor(0.5))\n",
    "         self.local_weight = nn.Parameter(torch.tensor(0.5))\n",
    "         self.dw_conv = nn.Conv2d(channels, channels, 3, \n",
    "                               padding=1, groups=channels)\n",
    " \n",
    "     def forward(self, local, global_feat):\n",
    "         # 通道注意力增强局部特征\n",
    "         local_att = self.channel_att(local) * local\n",
    "         # 空间注意力增强全局特征\n",
    "         global_att = self.spatial_att(global_feat) * global_feat\n",
    "         # 动态权重融合\n",
    "         w_g = torch.sigmoid(self.global_weight)\n",
    "         w_l = torch.sigmoid(self.local_weight)\n",
    "         fused = self.dw_conv(w_g*global_att + w_l*local_att)\n",
    "         return fused\n",
    "\n",
    "# --------------------------- 修改层级融合模块 ---------------------------\n",
    "class HierarchicalFusion(nn.Module):\n",
    "    def __init__(self, in_channels, d_model):\n",
    "        super().__init__()\n",
    "        self.local_branch = nn.Sequential(\n",
    "            DeformConv2d(in_channels, d_model, 3, padding=1),\n",
    "            nn.BatchNorm2d(d_model),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.global_branch = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, d_model, 1),\n",
    "            nn.BatchNorm2d(d_model),\n",
    "            nn.ReLU(inplace=True),\n",
    "            TransformerUnit(d_model)\n",
    "        )\n",
    "\n",
    "        self.adaptive_fusion = AdaptiveFusion(d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        local = self.local_branch(x)\n",
    "        global_feat = self.global_branch(x)\n",
    "\n",
    "        return self.adaptive_fusion(local, global_feat)\n",
    "\n",
    "# --------------------------- 新增注意力子模块 ---------------------------\n",
    "class ChannelAttention(nn.Module):\n",
    "    def __init__(self, channels, reduction=4):\n",
    "        super().__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(channels, max(8, channels//reduction)),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(max(8, channels//reduction), channels),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c = x.size()[:2]\n",
    "        avg = self.avg_pool(x).view(b, c)\n",
    "        max_val = self.max_pool(x).view(b, c)\n",
    "        weight = self.fc(avg + max_val).view(b, c, 1, 1)\n",
    "        return x * weight\n",
    "\n",
    "class SpatialAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(2, 1, 3, padding=1, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
    "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
    "        return x * self.conv(torch.cat([avg_out, max_out], dim=1))\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------- AMFT 主干模型 ---------------------------\n",
    "class AMFT(nn.Module):\n",
    "    def __init__(self, num_classes=7, alpha=0.75, d_model=128):\n",
    "        super().__init__()\n",
    "\n",
    "        # 使用alpha参数构建MobileNetV2主干\n",
    "        self.backbone = self._build_mobilenet_backbone(alpha)\n",
    "        fusion_indices = [3, 6, 9]  # 在指定层后添加融合点\n",
    "        fusion_channels = [self._get_output_channels(self.backbone[idx]) for idx in fusion_indices]\n",
    "\n",
    "        self.fusion_layers = nn.ModuleList([\n",
    "            HierarchicalFusion(c, d_model) for c in fusion_channels\n",
    "        ])\n",
    "\n",
    "        self.fusion_linear = nn.Linear(len(fusion_indices), len(fusion_indices))\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.AdaptiveAvgPool2d(1),\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(d_model, num_classes)\n",
    "        )\n",
    "\n",
    "    def _build_mobilenet_backbone(self, alpha):\n",
    "        \"\"\"根据alpha参数构建并缩放MobileNetV2主干\"\"\"\n",
    "        backbone = mobilenet_v2(weights=MobileNet_V2_Weights.IMAGENET1K_V1).features\n",
    "        # 移除索引10及之后的层（高层特征模拟器）\n",
    "        backbone = nn.Sequential(*list(backbone.children())[:10])  # 保留0-9层\n",
    "        \n",
    "        def scale_channels(channels):\n",
    "            return max(8, int(math.ceil(channels * alpha / 8)) * 8)\n",
    "        \n",
    "        for name, layer in backbone.named_children():\n",
    "            if isinstance(layer, nn.Conv2d):\n",
    "                original_out_channels = layer.out_channels\n",
    "                layer.out_channels = scale_channels(original_out_channels)\n",
    "            elif hasattr(layer, 'conv'):\n",
    "                for sub_layer_name, sub_layer in layer.conv.named_children():\n",
    "                    if isinstance(sub_layer, nn.Conv2d):\n",
    "                        original_in_channels = sub_layer.in_channels\n",
    "                        original_out_channels = sub_layer.out_channels\n",
    "                        sub_layer.in_channels = scale_channels(original_in_channels)\n",
    "                        sub_layer.out_channels = scale_channels(original_out_channels)\n",
    "        return backbone\n",
    "    \n",
    "    @staticmethod\n",
    "    def _get_output_channels(layer):\n",
    "        if hasattr(layer, 'out_channels'):\n",
    "            return layer.out_channels\n",
    "        elif hasattr(layer, 'conv') and isinstance(layer.conv[-1], nn.Conv2d):\n",
    "            return layer.conv[-1].out_channels\n",
    "        else:\n",
    "            raise ValueError(\"无法确定层的输出通道数\")\n",
    "\n",
    "    def forward(self, x):\n",
    "        fusion_outputs = []\n",
    "        current_fusion_idx = 0\n",
    "\n",
    "        for i, layer in enumerate(self.backbone):\n",
    "            x = layer(x)\n",
    "            if i in [3, 6, 9]:  # 在指定层后执行融合\n",
    "                fused = self.fusion_layers[current_fusion_idx](x)\n",
    "                fusion_outputs.append(fused)\n",
    "                current_fusion_idx += 1\n",
    "\n",
    "        final_feat = self._aggregate_features(fusion_outputs)\n",
    "        return self.classifier(final_feat)\n",
    "\n",
    "    def _aggregate_features(self, features):\n",
    "        resized_features = []\n",
    "        target_size = features[-1].shape[-2:]\n",
    "\n",
    "        for feat in features:\n",
    "            if feat.shape[-2:] != target_size:\n",
    "                feat = nn.functional.interpolate(feat, size=target_size, mode='bilinear', align_corners=False)\n",
    "            resized_features.append(feat)\n",
    "\n",
    "        spatial_att = torch.cat([f.mean(dim=1, keepdim=True) for f in resized_features], dim=1)\n",
    "        spatial_att = nn.AdaptiveAvgPool2d((1, 1))(spatial_att)\n",
    "        spatial_att = spatial_att.view(spatial_att.size(0), -1)\n",
    "\n",
    "        spatial_att = self.fusion_linear(spatial_att)\n",
    "        spatial_att = nn.Softmax(dim=1)(spatial_att)\n",
    "\n",
    "        spatial_att = spatial_att.unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)  # [B, N, 1, 1, 1]\n",
    "        spatial_att = spatial_att.permute(1, 0, 2, 3, 4)  # [N, B, 1, 1, 1]\n",
    "\n",
    "        stacked = torch.stack(resized_features)  # [N, B, C, H, W]\n",
    "\n",
    "        weighted_sum = torch.sum(stacked * spatial_att, dim=0)  # [B, C, H, W]\n",
    "\n",
    "        return weighted_sum"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
